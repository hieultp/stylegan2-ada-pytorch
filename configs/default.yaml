name: bedroom_256_boundary
state: train # train/test/debug
seed: 42

# path to original working directory
# hydra hijacks working directory by changing it to the current log directory,
# so it's useful to have this path as a special variable
# https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
work_dir: ${hydra:runtime.cwd}

logging:
  root: ./logs

dataset:
  root: /home/ubuntu/bedroom_256_train/
  batch_size: 64
  num_workers: 8
  pin_memory: True
  val_size: 1076 # 1076 over 56076

model:
  pretrained: False
  sync_dist: True # Use when training with multiple gpu

  monitor: val/loss
  optimizer:
    lr: 1.e-4
    weight_decay: 1.e-4
  lr_scheduler:
    mode: min
    factor: 0.5
    patience: 10
    threshold: 1.e-4
    threshold_mode: rel
    cooldown: 0
    min_lr: 1.e-8
    verbose: True

trainer:
  # GPU related
  precision: 16
  gpus: -1
  num_nodes: 1
  strategy: ddp
  benchmark: True
  sync_batchnorm: True

  # Training related
  max_steps: 10000
  # limit_train_batches: 1.0
  # gradient_clip_val: 0.1 # gradient clipping max norm
  # gradient_clip_algorithm: "norm"

# Logging, progress bar
refresh_rate: 1

model_ckpt:
  dirpath: ckpts/
  filename: "vgg19_bn-epoch{epoch}-step{step}"
  monitor: ${model.monitor}
  save_last: True
  save_top_k: 4
  mode: min
  auto_insert_metric_name: False

ddp_plugin:
  # These two args only work with accelerator = "ddp"
  find_unused_params: False
  # fp16_hook: True

hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}-${name}
